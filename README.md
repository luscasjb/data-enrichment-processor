# Data Enrichment Streaming Project

## Summary

This project represents a **key component within a larger message processing flow**. Acting as a real-time data enrichment service (streaming), its primary purpose is to intercept a specific stage of this pipeline to consume events, cross-reference and validate this information with data from multiple databases, and finally, produce a consolidated message for the **next phase of the flow** in a new Apache Kafka topic.

The service is designed to be resilient and follows the Single Responsibility Principle, decoupling the responsibilities of consumption, processing, data access, and message production, ensuring its maintainability and scalability within the overall architecture.

## Data Flow and Architecture

The processing flow of a message follows these steps:

1.  **Event Consumption (CDC)**: The service subscribes to the Kafka topic `mysql.local.listener`, which receives events generated by Debezium from changes in the `movements` table of a MySQL database.

2.  **Relevance Validation**: For each consumed event, the service verifies if it corresponds to the **latest and most recent state** of a `request_id`. Events that do not represent the final status (e.g., arriving out of order) are discarded to ensure consistency.

3.  **Situation-Based Routing Logic**: Based on the movement's situation, the flow is directed:
    * **Situations `PD` (Pending) or `IP` (In Progress)**: Trigger the enrichment process.
    * **Situations `CP` (Completed) or `CN` (Cancelled)**: Trigger the sending of a "Tombstone" message (with a null value) to the final topic. This is a Kafka pattern to signal the removal or invalidation of the key (`request_id`).

4.  **Enrichment Process**:
    * The service queries the `people_requests` table (MySQL) to obtain a `person_id` from the `request_id`.
    * With the `person_id`, it queries the `people` table in a **PostgreSQL** database to obtain the person's `name`.
    * The situation code (e.g., `PD`) is mapped to a readable description (e.g., `PENDING`).

5.  **Final Message Production**:
    * A new message, in the `EnrichedData` format, is constructed containing the original and enriched data (`request_id`, `person_id`, `person_name`, `status_description`).
    * This message is serialized and sent to the Kafka topic `final_enriched_data`, using the `request_id` as the message key to ensure correct partitioning.

## Main Components

The application is divided into the following components:

* **`listener`**: Solely responsible for consuming messages from the source topic and delegating processing.
* **`service`**: Orchestrates the business flow, applying validation and routing rules.
* **`repository`**: Abstraction layer for data access, with specific implementations for MySQL and PostgreSQL databases.
* **`mapper`**: Performs data transformations, such as converting situation codes to their descriptions.
* **`producer`**: Encapsulates the logic for building and sending messages (both enriched and "tombstones") to the destination Kafka topic.

## Technologies Used

* Java & Spring Boot
* Apache Kafka (Consumer and Producer)
* Debezium (for Change Data Capture)
* MySQL (as a data source)
* PostgreSQL (as a data source for enrichment)
* JDBC Template